{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DS 301: Applied Data Modeling and Predictive Analysis**\n",
    "\n",
    "# Lab 8 â€“ PCA\n",
    "\n",
    "Nok Wongpiromsarn, 16 October 2020\n",
    "\n",
    "**Credit:** https://github.com/asukul/DS301-f19/blob/master/Lab_6_PCA_Fashion_MNIST.ipynb by Adisak Sukul\n",
    "\n",
    "**Instructions:**\n",
    "1. Construct the training and test sets: X_train, y_train, X_test, y_test from the fashion_mnist (Links to an external site.) dataset. Reshape X_train such that it becomes a 2D array.\n",
    "2. Project X_train onto the hyperplane defined by the first d = 4 principal components. Make sure you scale your features.\n",
    "3. Display explained_variance_ratio_  of the PCA obtained from step 2.\n",
    "4. Compute and display the number of principal components required to obtain\n",
    "   - 25% variance\n",
    "   - 50% variance\n",
    "   - 75% variance\n",
    "   - 95% variance\n",
    "5. Apply PCA to compress X_train such that 75% of its variance is preserved.\n",
    "6. Compared the size of the original X_train and the one obtained from step 5.\n",
    "7. Pick your favorite classifier. Compare the computation time of classifier.fit() between using the original dataset and using the compressed dataset obtained from step 5.\n",
    "8. Use the test set to compare the accuracy of the models generated in step 7 with and without applying PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Construct the training and test sets: X_train, y_train, X_test, y_test from the fashion_mnist (Links to an external site.) dataset. Reshape X_train such that it becomes a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# X_train is a 3D array with shape (60000, 28, 28). Reshape it to get a 2D array.\n",
    "# To do\n",
    "print(X_train.shape)\n",
    "print(X_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Project X_train onto the hyperplane defined by the first d = 4 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# First we need to scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Project X_scaled to the first 4 principal components\n",
    "# To-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_scaled[1].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Display explained_variance_ratio_  of the PCA obtained from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(pca.components_[0].reshape(28, 28), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compute and display the number of principal components required to obtain \n",
    "- 25% variance\n",
    "- 50% variance\n",
    "- 75% variance\n",
    "- 95% variance\n",
    "- 100% variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of principal components required to obtain\")\n",
    "print(\"  25% variance: {}\".format(d25))\n",
    "print(\"  50% variance: {}\".format(d50))\n",
    "print(\"  75% variance: {}\".format(d75))\n",
    "print(\"  95% variance: {}\".format(d95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply PCA to compress X_train such that 75% of its variance is preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compared the size of the original X_train and the one obtained from step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train size: {}\".format(X_scaled.nbytes))\n",
    "print(\"X_pca size: {}\".format(X_pca.nbytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indices = [1, 2]\n",
    "\n",
    "num_rows = len(plot_indices)\n",
    "f, axarr = plt.subplots(num_rows,2, figsize=(12, 9))\n",
    "\n",
    "for row in range(len(plot_indices)):\n",
    "    axarr[row, 0].imshow(X_scaled[plot_indices[row]].reshape(28, 28))\n",
    "    axarr[row, 1].imshow(X_recovered[plot_indices[row]].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Pick your favorite classifier. Compare the computation time of classifier.fit() between using the original dataset and using the compressed dataset obtained from step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_ori = LogisticRegression(solver = 'lbfgs', multi_class='auto')\n",
    "\n",
    "start_time = time.time()\n",
    "clf_ori.fit(X_scaled, y_train)\n",
    "end_time = time.time()\n",
    "print(\"Without PCA, take {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Use the test set to compare the accuracy of the models generated in step 7 with and without applying PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, 28 * 28))\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "y_pred_ori = clf_ori.predict(X_test_scaled)\n",
    "y_pred_pca = clf_pca.predict(X_test_pca)\n",
    "\n",
    "print(\"Accuracy without PCA: {}\".format(accuracy_score(y_test, y_pred_ori)))\n",
    "print(\"Accuracy with PCA:    {}\".format(accuracy_score(y_test, y_pred_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
